{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/of/BELLE/train/src')   # revise\n",
    "# sys.path.append('path/of/pixiu_private-main/train/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-26 14:59:13,975] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from utils import MultiClient\n",
    "ip = '...'\n",
    "base_port = 17860\n",
    "worker_addrs = [\n",
    "    f\"http://{ip}:{base_port + i}\" for i in range(8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiClient(worker_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIG = [\n",
    "    # int | float (numeric value between 0 and 1) in 'Temperature' Slider component\n",
    "    0.01,\n",
    "    # int | float (numeric value between 0 and 1) in 'Top p' Slider component\n",
    "    0.85,\n",
    "    # int | float (numeric value between 0 and 100) in 'Top k' Slider component\n",
    "    30,\n",
    "    # int | float (numeric value between 1 and 4) in 'Beams Number' Slider component\n",
    "    1,\n",
    "    # do sample\n",
    "    True,\n",
    "    # int | float (numeric value between 1 and 2000) in 'Max New Tokens' Slider component\n",
    "    1024,\n",
    "    # int | float (numeric value between 1 and 300) in 'Min New Tokens' Slider component\n",
    "    1,\n",
    "    # int | float (numeric value between 1.0 and 2.0) in 'Repetition Penalty' Slider component\n",
    "    1.2,\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础能力测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.read_json('/nfs/a100-006/hanweiguang/Projects/automatic_evaluate/eval_data.jsonl', lines=True)\n",
    "data = eval_df['instruction'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELLE template\n",
    "# template = \\\n",
    "# \"\"\"\"Human: \n",
    "# {text}\n",
    "\n",
    "# Assistant: \n",
    "# \"\"\"\n",
    "\n",
    "# glm template\n",
    "template = \\\n",
    "\"\"\"{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:22<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "tasks = []\n",
    "for i in range(len(data)):\n",
    "    tasks.append([template.format(text=data[i])] + GENERATION_CONFIG)\n",
    "answers = client.predict(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>gold_response</th>\n",
       "      <th>type</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>请你写出一个可以生成五个不同随机数的 Python 程序。\\n</td>\n",
       "      <td>要生成五个不同的随机数，可以使用 Python 的 random 模块中的 randint(...</td>\n",
       "      <td>code</td>\n",
       "      <td>以下是使用 Python 的 random 模块生成五个不同随机整数的示例代码:\\n\\n``...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>回答以下问题的答案。\\n中国有多少个省？</td>\n",
       "      <td>中国目前有34个省级行政区。具体分为23个省、5个自治区、4个直辖市和2个特别行政区。以下是...</td>\n",
       "      <td>open qa</td>\n",
       "      <td>中国有34个省级行政区,包括23个省、5个自治区、4个直辖市和2个特别行政区(香港和澳门)。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>什么是机器学习？它有哪些应用场景？\\n</td>\n",
       "      <td>机器学习是一种人工智能科学的分支，其目的是使计算机系统能够自动学习和改进，而不需要严格的编程...</td>\n",
       "      <td>open qa</td>\n",
       "      <td>机器学习是一种人工智能领域的技术,旨在使计算机程序从数据中学习和提取模式,并利用这些知识做出...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>提供一个主题，让学生进行头脑风暴，产生关于该主题的想法。\\n如何提高城市化水平。</td>\n",
       "      <td>如何提高城市化水平？\\n头脑风暴产生的想法：\\n1. 投资基础设施：城市化的基础是基础设施。...</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>以下是一个主题,可以激发学生的头脑风暴:\\n\\n\"创新城市化解决方案——促进可持续发展和提高...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>针对给定的主题，进行头脑风暴并记录所有想法。\\n如何提高公司的销售额？</td>\n",
       "      <td>提高公司的销售额是一个涉及到多种因素的问题。下面是一些可能有用的头脑风暴提示：\\n1. 提供...</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>以下是一些可能有助于提高公司销售额的想法:\\n\\n1. 打造一个独特的品牌形象,包括标志、标...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                               instruction  \\\n",
       "0   1           请你写出一个可以生成五个不同随机数的 Python 程序。\\n   \n",
       "1   2                      回答以下问题的答案。\\n中国有多少个省？   \n",
       "2   3                       什么是机器学习？它有哪些应用场景？\\n   \n",
       "3   4  提供一个主题，让学生进行头脑风暴，产生关于该主题的想法。\\n如何提高城市化水平。   \n",
       "4   5       针对给定的主题，进行头脑风暴并记录所有想法。\\n如何提高公司的销售额？   \n",
       "\n",
       "                                       gold_response           type  \\\n",
       "0  要生成五个不同的随机数，可以使用 Python 的 random 模块中的 randint(...           code   \n",
       "1  中国目前有34个省级行政区。具体分为23个省、5个自治区、4个直辖市和2个特别行政区。以下是...        open qa   \n",
       "2  机器学习是一种人工智能科学的分支，其目的是使计算机系统能够自动学习和改进，而不需要严格的编程...        open qa   \n",
       "3  如何提高城市化水平？\\n头脑风暴产生的想法：\\n1. 投资基础设施：城市化的基础是基础设施。...  brainstorming   \n",
       "4  提高公司的销售额是一个涉及到多种因素的问题。下面是一些可能有用的头脑风暴提示：\\n1. 提供...  brainstorming   \n",
       "\n",
       "                                            response  \n",
       "0  以下是使用 Python 的 random 模块生成五个不同随机整数的示例代码:\\n\\n``...  \n",
       "1     中国有34个省级行政区,包括23个省、5个自治区、4个直辖市和2个特别行政区(香港和澳门)。  \n",
       "2  机器学习是一种人工智能领域的技术,旨在使计算机程序从数据中学习和提取模式,并利用这些知识做出...  \n",
       "3  以下是一个主题,可以激发学生的头脑风暴:\\n\\n\"创新城市化解决方案——促进可持续发展和提高...  \n",
       "4  以下是一些可能有助于提高公司销售额的想法:\\n\\n1. 打造一个独特的品牌形象,包括标志、标...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['response'] = answers\n",
    "eval_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_json('/nfs/a100-006/hanweiguang/Projects/automatic_evaluate/data/chatglm2-6b.json', lines=True, orient='records', force_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 博学测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/nfs/a100-006/hanweiguang/Projects/BELLE/data/boxue/exam_1.jsonl\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(val) for val in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \\\n",
    "\"\"\"Human: {type}\n",
    "{question}\n",
    "{candidates}\n",
    "\n",
    "Assistant: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for i in range(len(data)):\n",
    "    sample = data[i]\n",
    "    tasks.append([template.format(\n",
    "        question=sample['question'].strip(),\n",
    "        candidates='\\n'.join(sample['candidates']),\n",
    "        type=sample['type']\n",
    "    )] + GENERATION_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = client.predict(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preds = pd.DataFrame({\"prompt\": [task[0] for task in tasks], \"pred\": answers, \"labels\": [sample[\"answer\"] for sample in data]})\n",
    "preds.to_excel('../../data/boxue/pred_1.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:11<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/nfs/v100-022/xytian/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/nfs/v100-022/xytian/chatglm2-6b', trust_remote_code=True)\n",
    "model = model.half().cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.bos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。',\n",
       " [('你好', '你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "response, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.01,\n",
    "    top_p=0.85,\n",
    "    top_k=30,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=1024,  # max_length=max_new_tokens+input_sequence\n",
    "    min_new_tokens=1,  # min_length=min_new_tokens+input_sequence\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    ")\n",
    "model.chat(tokenizer, \"你好\", **generation_config.to_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}