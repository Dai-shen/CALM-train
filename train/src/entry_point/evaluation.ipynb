{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('path/of/BELLE/train/src')   # revise\n",
    "# sys.path.append('path/of/pixiu_private-main/train/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-26 14:59:13,975] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from utils import MultiClient\n",
    "ip = '...'\n",
    "base_port = 17860\n",
    "worker_addrs = [\n",
    "    f\"http://{ip}:{base_port + i}\" for i in range(8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiClient(worker_addrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_CONFIG = [\n",
    "    # int | float (numeric value between 0 and 1) in 'Temperature' Slider component\n",
    "    0.01,\n",
    "    # int | float (numeric value between 0 and 1) in 'Top p' Slider component\n",
    "    0.85,\n",
    "    # int | float (numeric value between 0 and 100) in 'Top k' Slider component\n",
    "    30,\n",
    "    # int | float (numeric value between 1 and 4) in 'Beams Number' Slider component\n",
    "    1,\n",
    "    # do sample\n",
    "    True,\n",
    "    # int | float (numeric value between 1 and 2000) in 'Max New Tokens' Slider component\n",
    "    1024,\n",
    "    # int | float (numeric value between 1 and 300) in 'Min New Tokens' Slider component\n",
    "    1,\n",
    "    # int | float (numeric value between 1.0 and 2.0) in 'Repetition Penalty' Slider component\n",
    "    1.2,\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŸºç¡€èƒ½åŠ›æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_df = pd.read_json('/nfs/a100-006/hanweiguang/Projects/automatic_evaluate/eval_data.jsonl', lines=True)\n",
    "data = eval_df['instruction'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELLE template\n",
    "# template = \\\n",
    "# \"\"\"\"Human: \n",
    "# {text}\n",
    "\n",
    "# Assistant: \n",
    "# \"\"\"\n",
    "\n",
    "# glm template\n",
    "template = \\\n",
    "\"\"\"{text}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [17:22<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "tasks = []\n",
    "for i in range(len(data)):\n",
    "    tasks.append([template.format(text=data[i])] + GENERATION_CONFIG)\n",
    "answers = client.predict(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>instruction</th>\n",
       "      <th>gold_response</th>\n",
       "      <th>type</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>è¯·ä½ å†™å‡ºä¸€ä¸ªå¯ä»¥ç”Ÿæˆäº”ä¸ªä¸åŒéšæœºæ•°çš„ Python ç¨‹åºã€‚\\n</td>\n",
       "      <td>è¦ç”Ÿæˆäº”ä¸ªä¸åŒçš„éšæœºæ•°ï¼Œå¯ä»¥ä½¿ç”¨ Python çš„ random æ¨¡å—ä¸­çš„ randint(...</td>\n",
       "      <td>code</td>\n",
       "      <td>ä»¥ä¸‹æ˜¯ä½¿ç”¨ Python çš„ random æ¨¡å—ç”Ÿæˆäº”ä¸ªä¸åŒéšæœºæ•´æ•°çš„ç¤ºä¾‹ä»£ç :\\n\\n``...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>å›ç­”ä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆã€‚\\nä¸­å›½æœ‰å¤šå°‘ä¸ªçœï¼Ÿ</td>\n",
       "      <td>ä¸­å›½ç›®å‰æœ‰34ä¸ªçœçº§è¡Œæ”¿åŒºã€‚å…·ä½“åˆ†ä¸º23ä¸ªçœã€5ä¸ªè‡ªæ²»åŒºã€4ä¸ªç›´è¾–å¸‚å’Œ2ä¸ªç‰¹åˆ«è¡Œæ”¿åŒºã€‚ä»¥ä¸‹æ˜¯...</td>\n",
       "      <td>open qa</td>\n",
       "      <td>ä¸­å›½æœ‰34ä¸ªçœçº§è¡Œæ”¿åŒº,åŒ…æ‹¬23ä¸ªçœã€5ä¸ªè‡ªæ²»åŒºã€4ä¸ªç›´è¾–å¸‚å’Œ2ä¸ªç‰¹åˆ«è¡Œæ”¿åŒº(é¦™æ¸¯å’Œæ¾³é—¨)ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿå®ƒæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ\\n</td>\n",
       "      <td>æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ç§‘å­¦çš„åˆ†æ”¯ï¼Œå…¶ç›®çš„æ˜¯ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œä¸éœ€è¦ä¸¥æ ¼çš„ç¼–ç¨‹...</td>\n",
       "      <td>open qa</td>\n",
       "      <td>æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ€æœ¯,æ—¨åœ¨ä½¿è®¡ç®—æœºç¨‹åºä»æ•°æ®ä¸­å­¦ä¹ å’Œæå–æ¨¡å¼,å¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†åšå‡º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>æä¾›ä¸€ä¸ªä¸»é¢˜ï¼Œè®©å­¦ç”Ÿè¿›è¡Œå¤´è„‘é£æš´ï¼Œäº§ç”Ÿå…³äºè¯¥ä¸»é¢˜çš„æƒ³æ³•ã€‚\\nå¦‚ä½•æé«˜åŸå¸‚åŒ–æ°´å¹³ã€‚</td>\n",
       "      <td>å¦‚ä½•æé«˜åŸå¸‚åŒ–æ°´å¹³ï¼Ÿ\\nå¤´è„‘é£æš´äº§ç”Ÿçš„æƒ³æ³•ï¼š\\n1. æŠ•èµ„åŸºç¡€è®¾æ–½ï¼šåŸå¸‚åŒ–çš„åŸºç¡€æ˜¯åŸºç¡€è®¾æ–½ã€‚...</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¸»é¢˜,å¯ä»¥æ¿€å‘å­¦ç”Ÿçš„å¤´è„‘é£æš´:\\n\\n\"åˆ›æ–°åŸå¸‚åŒ–è§£å†³æ–¹æ¡ˆâ€”â€”ä¿ƒè¿›å¯æŒç»­å‘å±•å’Œæé«˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>é’ˆå¯¹ç»™å®šçš„ä¸»é¢˜ï¼Œè¿›è¡Œå¤´è„‘é£æš´å¹¶è®°å½•æ‰€æœ‰æƒ³æ³•ã€‚\\nå¦‚ä½•æé«˜å…¬å¸çš„é”€å”®é¢ï¼Ÿ</td>\n",
       "      <td>æé«˜å…¬å¸çš„é”€å”®é¢æ˜¯ä¸€ä¸ªæ¶‰åŠåˆ°å¤šç§å› ç´ çš„é—®é¢˜ã€‚ä¸‹é¢æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„å¤´è„‘é£æš´æç¤ºï¼š\\n1. æä¾›...</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰åŠ©äºæé«˜å…¬å¸é”€å”®é¢çš„æƒ³æ³•:\\n\\n1. æ‰“é€ ä¸€ä¸ªç‹¬ç‰¹çš„å“ç‰Œå½¢è±¡,åŒ…æ‹¬æ ‡å¿—ã€æ ‡...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                               instruction  \\\n",
       "0   1           è¯·ä½ å†™å‡ºä¸€ä¸ªå¯ä»¥ç”Ÿæˆäº”ä¸ªä¸åŒéšæœºæ•°çš„ Python ç¨‹åºã€‚\\n   \n",
       "1   2                      å›ç­”ä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆã€‚\\nä¸­å›½æœ‰å¤šå°‘ä¸ªçœï¼Ÿ   \n",
       "2   3                       ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿå®ƒæœ‰å“ªäº›åº”ç”¨åœºæ™¯ï¼Ÿ\\n   \n",
       "3   4  æä¾›ä¸€ä¸ªä¸»é¢˜ï¼Œè®©å­¦ç”Ÿè¿›è¡Œå¤´è„‘é£æš´ï¼Œäº§ç”Ÿå…³äºè¯¥ä¸»é¢˜çš„æƒ³æ³•ã€‚\\nå¦‚ä½•æé«˜åŸå¸‚åŒ–æ°´å¹³ã€‚   \n",
       "4   5       é’ˆå¯¹ç»™å®šçš„ä¸»é¢˜ï¼Œè¿›è¡Œå¤´è„‘é£æš´å¹¶è®°å½•æ‰€æœ‰æƒ³æ³•ã€‚\\nå¦‚ä½•æé«˜å…¬å¸çš„é”€å”®é¢ï¼Ÿ   \n",
       "\n",
       "                                       gold_response           type  \\\n",
       "0  è¦ç”Ÿæˆäº”ä¸ªä¸åŒçš„éšæœºæ•°ï¼Œå¯ä»¥ä½¿ç”¨ Python çš„ random æ¨¡å—ä¸­çš„ randint(...           code   \n",
       "1  ä¸­å›½ç›®å‰æœ‰34ä¸ªçœçº§è¡Œæ”¿åŒºã€‚å…·ä½“åˆ†ä¸º23ä¸ªçœã€5ä¸ªè‡ªæ²»åŒºã€4ä¸ªç›´è¾–å¸‚å’Œ2ä¸ªç‰¹åˆ«è¡Œæ”¿åŒºã€‚ä»¥ä¸‹æ˜¯...        open qa   \n",
       "2  æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½ç§‘å­¦çš„åˆ†æ”¯ï¼Œå…¶ç›®çš„æ˜¯ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œä¸éœ€è¦ä¸¥æ ¼çš„ç¼–ç¨‹...        open qa   \n",
       "3  å¦‚ä½•æé«˜åŸå¸‚åŒ–æ°´å¹³ï¼Ÿ\\nå¤´è„‘é£æš´äº§ç”Ÿçš„æƒ³æ³•ï¼š\\n1. æŠ•èµ„åŸºç¡€è®¾æ–½ï¼šåŸå¸‚åŒ–çš„åŸºç¡€æ˜¯åŸºç¡€è®¾æ–½ã€‚...  brainstorming   \n",
       "4  æé«˜å…¬å¸çš„é”€å”®é¢æ˜¯ä¸€ä¸ªæ¶‰åŠåˆ°å¤šç§å› ç´ çš„é—®é¢˜ã€‚ä¸‹é¢æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„å¤´è„‘é£æš´æç¤ºï¼š\\n1. æä¾›...  brainstorming   \n",
       "\n",
       "                                            response  \n",
       "0  ä»¥ä¸‹æ˜¯ä½¿ç”¨ Python çš„ random æ¨¡å—ç”Ÿæˆäº”ä¸ªä¸åŒéšæœºæ•´æ•°çš„ç¤ºä¾‹ä»£ç :\\n\\n``...  \n",
       "1     ä¸­å›½æœ‰34ä¸ªçœçº§è¡Œæ”¿åŒº,åŒ…æ‹¬23ä¸ªçœã€5ä¸ªè‡ªæ²»åŒºã€4ä¸ªç›´è¾–å¸‚å’Œ2ä¸ªç‰¹åˆ«è¡Œæ”¿åŒº(é¦™æ¸¯å’Œæ¾³é—¨)ã€‚  \n",
       "2  æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½é¢†åŸŸçš„æŠ€æœ¯,æ—¨åœ¨ä½¿è®¡ç®—æœºç¨‹åºä»æ•°æ®ä¸­å­¦ä¹ å’Œæå–æ¨¡å¼,å¹¶åˆ©ç”¨è¿™äº›çŸ¥è¯†åšå‡º...  \n",
       "3  ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¸»é¢˜,å¯ä»¥æ¿€å‘å­¦ç”Ÿçš„å¤´è„‘é£æš´:\\n\\n\"åˆ›æ–°åŸå¸‚åŒ–è§£å†³æ–¹æ¡ˆâ€”â€”ä¿ƒè¿›å¯æŒç»­å‘å±•å’Œæé«˜...  \n",
       "4  ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰åŠ©äºæé«˜å…¬å¸é”€å”®é¢çš„æƒ³æ³•:\\n\\n1. æ‰“é€ ä¸€ä¸ªç‹¬ç‰¹çš„å“ç‰Œå½¢è±¡,åŒ…æ‹¬æ ‡å¿—ã€æ ‡...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['response'] = answers\n",
    "eval_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_json('/nfs/a100-006/hanweiguang/Projects/automatic_evaluate/data/chatglm2-6b.json', lines=True, orient='records', force_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åšå­¦æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/nfs/a100-006/hanweiguang/Projects/BELLE/data/boxue/exam_1.jsonl\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(val) for val in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \\\n",
    "\"\"\"Human: {type}\n",
    "{question}\n",
    "{candidates}\n",
    "\n",
    "Assistant: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for i in range(len(data)):\n",
    "    sample = data[i]\n",
    "    tasks.append([template.format(\n",
    "        question=sample['question'].strip(),\n",
    "        candidates='\\n'.join(sample['candidates']),\n",
    "        type=sample['type']\n",
    "    )] + GENERATION_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = client.predict(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preds = pd.DataFrame({\"prompt\": [task[0] for task in tasks], \"pred\": answers, \"labels\": [sample[\"answer\"] for sample in data]})\n",
    "preds.to_excel('../../data/boxue/pred_1.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:11<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/nfs/v100-022/xytian/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/nfs/v100-022/xytian/chatglm2-6b', trust_remote_code=True)\n",
    "model = model.half().cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.bos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚',\n",
       " [('ä½ å¥½', 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "response, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    temperature=0.01,\n",
    "    top_p=0.85,\n",
    "    top_k=30,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=1024,  # max_length=max_new_tokens+input_sequence\n",
    "    min_new_tokens=1,  # min_length=min_new_tokens+input_sequence\n",
    "    repetition_penalty=1.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    ")\n",
    "model.chat(tokenizer, \"ä½ å¥½\", **generation_config.to_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}